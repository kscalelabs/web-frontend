---
title: "Sound stream for humanoid with efficient tokenization"
description: "Improving inference speed of vision-language-action models for edge devices while preserving encoding power."
date: "February 12, 2025"
image: "/research/images/css-pattern1.png"
author: Benjamin Bolte and Paweł Budzianowski
---

Having an efficient end-to-end model that processes all types of modalities which include audio, video, and proprioception while outputting multiple actions is fundamental to effective deployments of robots around us.
Lately, we have shared our research on speeding up video-language-action models where actions are output at 50Hz.<sup id="fn-1">[1](#footnote-1)</sup>
In this blog, we want to share our results on speeding up processing of what is going on inside the humanoid brain,
with a focus on audio. Sound is important medium since we believe that voice will be the main communication channel
with robots and humanoids particularly. And as such, voice should be treated as a first-class citizen,
even if the robots might be more effectively yelled at thanks to it.<sup id="fn-1">[2](#footnote-1)</sup>

Just like with video input, this requires fast processing of a constant stream of audio to
pick up voice commands, full conversations, or surrounding noises to guide robots
around our houses and factories and translate our voice directly into robot actions.
Fast tokenization is a key ingredient in processing that amount of data.
Lately, look-up free quantization has opened up efficient tokenization of images and video.<sup id="fn-1">[3](#footnote-1)</sup>  
We share our research on expanding these efficient methods to audio so that the world model is able to process all sensors the way humans do.
If you want to try the tokenizer, we share K-Scale Speech Tokenizer family of models and accompanying [Github repository](https://github.com/kscalelabs/kst). 

## Pushing tokenization to the limits

Tokenizers allow processing of continuous world data into a discrete space that is
more suitable for neural network processing.
They are key building blocks of the modern machine learning stack and are used across image,
language, and robotics domains.<sup id="fn-1">[4](#footnote-1)</sup>
The key element to breaking down transformers' processing
of images is converting them into discrete tokens. We can achieve this through various methods
like patch-based tokenization, where images are divided into fixed-size patches and then embedded into tokens.
More advanced approaches use learned codebooks to map image features to discrete tokens,
enabling more efficient representation of visual information.
Successful architectures like ViT have shown that such tokenization can
effectively capture both local and global image features.

In the case of audio processing, we need to convert continuous waveforms into discrete
tokens that capture both acoustic and semantic information. This typically involves first converting the raw audio into spectrograms or mel-spectrograms, then applying vector quantization to map these continuous features into a discrete codebook space. The challenge lies in preserving both the acoustic quality and semantic meaning while maintaining computational efficiency.

Let's follow an example from SoundStorm and consider encoding audio information at $4000$
bits per second (bps).
If the tokenizer can pull and process around $320$ steps together per second,
processing audio at $16$ kHz produces $50$ frames per second. This corresponds to $4000/50 = 80$ bits allocated to each frame.
A standard vector quantizer would require storing $2^{80}$ codebooks, which is roughly comparable to the number of stars in the universe.

### Residual Vector Quantization

Residual Vector Quantizer (RVQ), cascades multiple layers of vector quantization.
The process begins by quantizing the input vector through the first quantizer. Then, the quantization residual (the difference between the input and its quantized version) is computed and passed through subsequent quantization layers. This cascading approach allows us to break down the massive codebook requirement into manageable pieces while maintaining high fidelity.
The graph below showcases $4$ levels, each with quantizer using a codebook of size $1024$.
Tokenizer with Residual Vector Quantizer.
Each number references specific codebook vector at particular level. 

<figure style={{ display: "flex", flexDirection: "column", alignItems: "center" }}>
  <img
    src="/research/images/rvq.png"
    alt="RVQ."
    style={{ width: "90%", display: "block", margin: "0 auto", marginBottom: "0.1rem" }}
  />
  <figcaption style={{ marginBottom: "1rem", textAlign: "center" }}>
    Tokenizer with Residual Vector Quantizer.
  </figcaption>
</figure>

### Lookup Free Quantization

Above approach requires typically from $4$ to $8$ levels of quantizers for
achieving high quality of compression. Look-up free quantization<sup id="fn-1">[3](#footnote-1)</sup>
represents a significant advancement in efficient video modeling by eliminating
the need for expensive codebook lookups. The key insight is reducing the VQ-VAE
codebook's embedding dimension to zero. Instead of maintaining a codebook $C$ of
$d$-dimensional embeddings of size $K$, we replace it with a simple integer set $C$ where $|C| = K$.
Traditional VQ-VAE models must compare encoder outputs against all $K$ $d$-dimensional
embeddings in the codebook to find the closest match. LFQ eliminates this computational burden by decomposing the latent space into independent dimensions. 
Look up free quantization opens up efficient video
modelling and shows promise for other modalities, including audio.

<figure style={{ display: "flex", flexDirection: "column", alignItems: "center" }}>
  <img src="/research/images/lfq.png" alt="RVQ." style={{ width: "70%", marginBottom: "0.1rem" }} />
  <figcaption style={{ marginBottom: "1rem", textAlign: "center" }}>
    {" "}
    Lookup Free Quantizer Architecture.
  </figcaption>
</figure>
Since LFQ is a single-layer quantizer, we operate at $8$ times smaller bit rate than with RVQ which
results in $500$ bps.{" "}

## K-Scale Speech Tokens (KST)

Given above inspiration we propose a fast and semantically informed tokenizer
for audio that is useful for transferring audio into humanoid actions.
We propose a family of different tokenizers ranging from 1 to 8 levels of
quantization and varying size of the codebooks.

### LFQ for Sound

Drawing inspiration from SoundStorm and SpeechTokenizer approach,
we combined Lookup-Free Quantization with encoder-decoder setup.
No code embeddings allows to improve the processing speed while maintaining
the similar architecture as in the image or video modalities.
KST - LFQ quantization with Semantic Alignment against HuBERT Tokens.

<figure style={{ display: "flex", flexDirection: "column", alignItems: "center" }}>
  <img src="/research/images/kst.png" alt="RVQ." width="50%" style={{ marginBottom: "0.1rem" }} />
  <figcaption style={{ marginBottom: "1rem", textAlign: "center" }}>
    {" "}
    KST - LFQ quantization with Semantic Alignment against HuBERT Tokens.
  </figcaption>
</figure>

## Aligning with Human-like Information

Drawing inspiration from SpeechTokenizer approach we guide
the training of the efficient tokenizer to be semantically rich.
This stems from the fact that interaction with robots ACTION:
explain that is about understanding not pure sound quality.
Robots needs to understand words and gneeral sound should be understandable.
HuBERT (Hidden-Unit BERT)<sup id="fn-1">[5](#footnote-1)</sup> is
a self-supervised model that learns speech representations by masking and predicting portions of the input signal. It creates semantically meaningful tokens by clustering similar speech patterns together, effectively capturing linguistic structure without explicit supervision. These tokens serve as an excellent target for knowledge distillation since they encode human-like understanding of speech.
Through knowledge distillation, we can transfer the semantic understanding capabilities of HuBERT to our more efficient KST model. By training KST to predict HuBERT tokens while maintaining its efficient architecture, we create a fast tokenizer that preserves the semantic richness needed for robot interactions. This approach balances computational efficiency with semantic understanding.

## Experiments

Different variants of KST model are trained on the Libri-Light dataset which offers 60k+ hours of unlabelled speech. It is the largest open spoken English audio suitable for training speech recognition systems under limited or no supervision. It is derived from open-source audio books
from the LibriVox project. For effective comparison, we have kept the original training parameters of the SpeechTokenizer. Efficient tokenization can be efficiently trained - we used two nodes of A$100$s over a span of $4$ days for each model. All models consists of around $100$M parameters. You can see the full training config at the KST repository.

### Evaluation

We first evaluate our tokenizer through sound reconstruction test -
we randomly sampled $500$ speech samples from the LJSpeech test set.
We focused on the semantic utility of the tokens by assessing content
accuracy through Word Error Rate (WER), obtained by transcribing the reconstructed speech.

{" "}

<figure style={{ display: "flex", flexDirection: "column", alignItems: "center" }}>
  <img
    src="/research/images/table_lfq.png"
    alt="RVQ."
    width="100%"
    style={{ marginBottom: "0.1rem" }}
  />
  <figcaption style={{ marginBottom: "1rem", textAlign: "center" }}>
    Speech reconstruction results.
  </figcaption>
</figure>

As anticipated, our proposed tokenization achieved a lower reconstruction quality compared to SpeechTokenizer.
However, owing to the single-layer quantization and the elimination of lookup operations,
the inference speed was improved. The qualitative analysis can be done by listening
to encoded and subsequently decoded audio examples from different:

<div className="flex flex-col md:flex-row items-center justify-center gap-4">
  <figure className="flex flex-col items-center text-center">
    <audio controls>
      <source src="/research/audio/lfq_gt.wav" type="audio/mpeg" />
      Your browser does not support the audio element.
    </audio>
    <figcaption>Ground Truth</figcaption>
  </figure>
  <figure className="flex flex-col items-center text-center">
    <audio controls>
      <source src="/research/audio/lfq_base.wav" type="audio/mpeg" />
      Your browser does not support the audio element.
    </audio>
    <figcaption>Tokenizer reconstruction</figcaption>
  </figure>
</div>

### Text-to-Speech

Efficiency in tokenization allows to propagate that across different speech domains.
We can add these encodings to the VLA model as another dimension.
You can also train ASR/Command models that can operate on the robot.
Finally, one can also train text-to-speech models with a simple design of
predicting one-level LFQ tokens given some prompt. We gave it a shot with a ridiculously small encoder-decoder architecture
comprising of $6$M parameters and around $2$ hours of artificially generated speech data
from just one voice actor. You can hear some samples here:

<div className="flex flex-col md:flex-row items-center justify-center gap-4">
  <figure className="flex flex-col items-center text-center">
    <audio controls>
      <source src="/research/audio/sample_1.wav" type="audio/mpeg" />
      Your browser does not support the audio element.
    </audio>
    <figcaption>Generated sample 1</figcaption>
  </figure>
  <figure className="flex flex-col items-center text-center">
    <audio controls>
      <source src="/research/audio/sample_2.wav" type="audio/mpeg" />
      Your browser does not support the audio element.
    </audio>
    <figcaption>Genereated sample 2</figcaption>
  </figure>
</div>

## Conclusions

Efficient tokenization across different modalities is fundamental to improving end-to-end approaches.
We have demonstrated that efficient tokenization of video can be transferred to audio modality.

## References

<div id="footnote-1">
[1] [Even if we yell at them.](https://yay-robot.github.io/)

[2] [EdgeVLA](https://kscale.dev/research/efficient-video-language-action-models-for-robots)

[3] [MagVIT2](https://magvit.cs.cmu.edu/v2/)

[4] See recent robotics tokenizers: [Cosmos](https://github.com/NVlabs/TokenBench) tokenizer or
[FAST tokenizer](https://www.physicalintelligence.company/research/fast)

[5] [HuBERT](https://ieeexplore.ieee.org/abstract/document/9585401)

[6] [SpeechTokenizer](https://arxiv.org/abs/2308.16692)

</div>
