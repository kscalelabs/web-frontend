---
title: "Discriminative, Learnable Semantic Tokenization"
description: "In this post, we describe a method for building an efficient codec for representing the semantics of continous signals, with a focus on audio."
date: "February 12, 2025"
author: Benjamin Bolte and Paweł Budzianowski
draft: true
---

At K-Scale, we are building the software and hardware to bring embodied intelligence into the real world. In particular, we are primarily focusing on machine learning systems which can run entirely at the edge. We think this is important for two reasons: first, running models entirely at the edge gives you full control over your own robot, without needing to worry that images or video from your home or business are being streamed back to a cloud somewhere, and second, it avoids the unit economics issues common with cloud-based AI systems, like Alexa, which might create an incentivize to make the robot dumber over time to save money instead of smarter over time to improve the product.[^edge-devices]

However, working with edge devices can be challenging, especially for people who want to experiment with incorporating [large multimodal models](https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/) into robotic systems. Bipedal robots in particular require fast processing of audio, video and proprioception data to able to smoothly interact with the world. To this end, several months ago we [shared our research](https://kscale.dev/research/efficient-video-language-action-models-for-robots) on speeding up video-language-action models through several architectural improvements that allowed us to build a closed-loop control system capable of running at 50 Hz. In this blog, we will share some of our recent work on building an efficient representation for [audio](https://yay-robot.github.io/). We make our code and models available in the accompanying [Github repository](https://github.com/kscalelabs/kst).

## Pushing Tokenization to the Limits

Tokenization is the process of converting a continous signal into a discrete representation. The discrete represention is typically a codec that encodes a [compressed representation](https://www.youtube.com/watch?v=AKMuA_TVz3A) of the original modality. Good codecs are able to [greatly improve model quality](https://arxiv.org/pdf/2112.10752) while simultaneously reducing model size, because they let models represent some manifold of the data relevant to the task at hand, rather than needing to represent the entire space. Building high-quality tokenizers has been a key ingredient in developing generative models for [images](https://arxiv.org/abs/2206.10789), [audio](https://arxiv.org/pdf/2106.07447) and [robotics](https://www.physicalintelligence.company/research/fast).

In the specific case of processing speech, we would like to convert continous waveforms into discrete tokens that can capture both acoustic and semantic information, while still being computationally efficient. This typically involves first converting the raw audio into spectrograms or mel-spectrograms, then applying vector quantization to map these continuous features into a discrete codebook space. The challenge lies in preserving both the acoustic quality and semantic meaning while maintaining computational efficiency.

### Residual Vector Quantization

[Residual Vector Quantizer (RVQ)](https://github.com/lucidrains/vector-quantize-pytorch) cascades multiple layers of vector quantization. The process begins by quantizing the input vector through the first quantizer. Then, the quantization residual (the difference between the input and its quantized version) is computed and passed through subsequent quantization layers. This cascading approach allows us to break down the massive codebook requirement into manageable pieces while maintaining high fidelity. The graph below showcases $4$ levels, each with quantizer using a codebook of size $1024$. Tokenizer with Residual Vector Quantizer. Each number references specific codebook vector at particular level. 

<figure style={{ display: "flex", flexDirection: "column", alignItems: "center" }}>
  <img
    src="/research/images/rvq.png"
    alt="RVQ."
    style={{ width: "90%", display: "block", margin: "0 auto", marginBottom: "0.1rem" }}
  />
  <figcaption style={{ marginBottom: "1rem", textAlign: "center" }}>
    Tokenizer with Residual Vector Quantizer.
  </figcaption>
</figure>

Let's follow an example from SoundStorm and consider encoding audio information at $4000$ bits per second (bps). If the tokenizer can pull and process around $320$ steps per second, processing audio at $16$ kHz produces $50$ frames per second. This corresponds to $4000/50 = 80$ bits allocated to each frame. A standard vector quantizer usually uses $2^{80}$ codebooks, which is approximately the number of grains of sand on Earth.

### Lookup-Free Quantization

The above approach typically requires from $4$ to $8$ levels of quantization to be able to learn a useful representation. [Lookup-free quantization](https://arxiv.org/abs/2310.05737) offers an alternative approach to building high-quality codebooks while eliminating the need for expensive codebook lookups. Instead of maintaining a codebook $C$ of $d$-dimensional embeddings of size $K$, LFQ represents the codebook with a simple integer set $C$ where $|C| = K$. While traditional VQ-VAE models need to compare encoder outputs against all $K$ $d$-dimensional embeddings in the codebook to find the closest match, LFQ eliminates this computational burden by decomposing the latent space into independent dimensions. 

<figure style={{ display: "flex", flexDirection: "column", alignItems: "center" }}>
  <img src="/research/images/lfq.png" alt="RVQ." style={{ width: "70%", marginBottom: "0.1rem" }} />
  <figcaption style={{ marginBottom: "1rem", textAlign: "center" }}>
    {" "}
    Lookup Free Quantizer Architecture.
  </figcaption>
</figure>

Since LFQ is a single-layer quantizer, we operate at $8$ times smaller bit rate than with RVQ which results in $500$ bps.

## K-Scale Speech Tokens (KST)

Given above inspiration we propose a fast and semantically informed tokenizer for audio that is useful for transferring audio into humanoid actions. We propose a family of different tokenizers ranging from 1 to 8 levels of quantization and varying size of the codebooks.

### LFQ for Audio

Drawing inspiration from the SoundStorm and SpeechTokenizer approaches, we combined Lookup-Free Quantization with encoder-decoder setup. No code embeddings allows to improve the processing speed while maintaining the similar architecture as in the image or video modalities. the similar architecture as in the image or video modalities. KST - LFQ quantization with Semantic Alignment against HuBERT Tokens.

<figure style={{ display: "flex", flexDirection: "column", alignItems: "center" }}>
  <img src="/research/images/kst.png" alt="RVQ." width="80%" style={{ marginBottom: "0.1rem" }} />
  <figcaption style={{ marginBottom: "1rem", textAlign: "center" }}>
    {" "}
    KST - LFQ quantization with Semantic Alignment against HuBERT Tokens.
  </figcaption>
</figure>

## Aligning with Human-like Information

Drawing inspiration from SpeechTokenizer approach, we guide the training of the efficient tokenizer to be semantically rich. This stems from the fact that robots should naturally understand both the sound of the interactions with the world as well as the semantic meaning of the human commands. [HuBERT (Hidden-Unit BERT)](https://ieeexplore.ieee.org/abstract/document/9585401) is a self-supervised model that learns speech representations by masking and predicting portions of the input signal. It creates semantically meaningful tokens by clustering similar speech patterns together, effectively capturing linguistic structure without explicit supervision. These tokens serve as an excellent target for knowledge distillation since they encode human-like understanding of speech. Through knowledge distillation, we can transfer the semantic understanding capabilities of HuBERT to our more efficient KST model. By training KST to predict HuBERT tokens while maintaining its efficient architecture, we create a fast tokenizer that preserves the semantic richness needed for robot interactions. This approach balances computational efficiency with semantic understanding.

## Experiments

Different variants of KST model are trained on the Libri-Light dataset which offers 60k+ hours of unlabelled speech. It is the largest open spoken English audio suitable for training speech recognition systems under limited or no supervision. It is derived from open-source audio books from the LibriVox project. For effective comparison, we have kept the original training parameters of the SpeechTokenizer. Efficient tokenization can be efficiently trained - we used two nodes of A$100$s over a span of $4$ days for each model. All models consists of around $100$M parameters. You can see the full training config at the KST repository.

### Evaluation

We first evaluate our tokenizer through sound reconstruction test - we randomly sampled $500$ speech samples from the LJSpeech test set. We focused on the semantic utility of the tokens by assessing content accuracy through Word Error Rate (WER), obtained by transcribing the reconstructed speech.

<figure style={{ display: "flex", flexDirection: "column", alignItems: "center" }}>
  <img
    src="/research/images/table_lfq.png"
    alt="RVQ."
    width="100%"
    style={{ marginBottom: "0.1rem" }}
  />
  <figcaption style={{ marginBottom: "1rem", textAlign: "center" }}>
    Speech reconstruction results.
  </figcaption>
</figure>

As anticipated, our proposed tokenization achieved a lower reconstruction quality compared to SpeechTokenizer. However, owing to the single-layer quantization and the elimination of lookup operations, the inference speed was improved. The qualitative analysis can be done by listening to encoded and subsequently decoded audio examples from different:

<div className="flex flex-col md:flex-row items-center justify-center gap-4">
  <figure className="flex flex-col items-center text-center">
    <audio controls>
      <source src="/research/audio/lfq_gt.wav" type="audio/mpeg" />
      Your browser does not support the audio element.
    </audio>
    <figcaption>Ground Truth</figcaption>
  </figure>
  <figure className="flex flex-col items-center text-center">
    <audio controls>
      <source src="/research/audio/lfq_base.wav" type="audio/mpeg" />
      Your browser does not support the audio element.
    </audio>
    <figcaption>Tokenizer reconstruction</figcaption>
  </figure>
</div>

### Text-to-Speech

Efficiency in tokenization allows to propagate that across different speech domains. We can add these encodings to the VLA model as another dimension. You can also train ASR/Command models that can operate on the robot. Finally, one can also train text-to-speech models with a simple design of predicting one-level LFQ tokens given some prompt. We gave it a shot with a ridiculously small encoder-decoder architecture comprising of $6$M parameters and around $2$ hours of artificially generated speech data from just one voice actor. You can hear some samples here:

<div className="flex flex-col md:flex-row items-center justify-center gap-4">
  <figure className="flex flex-col items-center text-center">
    <audio controls>
      <source src="/research/audio/sample_1.wav" type="audio/mpeg" />
      Your browser does not support the audio element.
    </audio>
    <figcaption>Generated sample 1</figcaption>
  </figure>
  <figure className="flex flex-col items-center text-center">
    <audio controls>
      <source src="/research/audio/sample_2.wav" type="audio/mpeg" />
      Your browser does not support the audio element.
    </audio>
    <figcaption>Generated sample 2</figcaption>
  </figure>
</div>

## Conclusions

Efficient tokenization across different modalities is fundamental to improving end-to-end approaches. We have demonstrated that efficient tokenization of video can be transferred to audio modality.

[^edge-devices]: As an open-source company, we hope you will use your robots in whatever way you see fit, including controlling it with powerful cloud-based models; however, we think it's important that the out-of-the-box product is a good, stand-alone unit.

[^magvit-2]: [MagVIT2](https://magvit.cs.cmu.edu/v2/)

[^fast-tokenizer]: See recent robotics tokenizers like [Cosmos](https://github.com/NVlabs/TokenBench) or [FAST tokenizer](https://www.physicalintelligence.company/research/fast)

[^speech-tokenizer]: [SpeechTokenizer](https://arxiv.org/abs/2308.16692)
