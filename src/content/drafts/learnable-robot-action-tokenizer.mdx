---
title: "Learnable Robot Action Tokenizer"
description: "Learnable Robot Action Tokenizer"
date: "February 17, 2025"
image: "/images/research/css-pattern1.png"
author: Paweł Budzianowski
---

In our last [blog post](https://www.kscale.dev/research/soundstream-humanoid-efficient-tokenization), we shared updates on efficient and learnable speech tokenization. However, our tokenization paradigm extends beyond just audio. To truly build robots that interact with the world seamlessly, we need to efficiently process and understand not just what they hear, but also how they move. Robot action tokenization is fundamental to achieving this, especially for high-frequency dexterous tasks. As recent works like FAST and MiniVLA have shown¹, the choice of tokenizer can have a significant impact on the performance of the final robot model. Here, we want to share our early research on bringing the same efficiency and learnability we explored in audio to the realm of robot actions. Just as voice commands will be a crucial communication channel, precise and nuanced robot actions are the language through which robots interact with the physical world. And just like with audio, fast and efficient tokenization is key to enabling robots to react in real-time to complex and dynamic environments.

## Look-up Free Quantization Revisited

We have previously introduced Lookup-Free Quantization (LFQ) as a powerful technique for efficient video and image modeling. LFQ offers a way to bypass the computationally expensive codebook lookups in traditional Vector Quantization (VQ) methods. This is achieved by effectively reducing the embedding dimension of the VQ codebook to zero, replacing it with a simple integer set. This drastically speeds up processing and makes tokenization more scalable, crucial for handling the high data throughput in real-time robotics.

## Robot Actions

Robot actions, particularly in dexterous manipulation, are often represented as continuous control signals — think joint torques, velocities, or end-effector positions. For a robot to perform complex tasks, especially at high frequencies required for smooth and responsive control, we need to efficiently encode these continuous action spaces into a discrete token space that can be understood and generated by neural networks. The challenge is to capture the nuances and complexities of robot movements while maintaining computational efficiency.

<figure style={{ display: "flex", flexDirection: "column", alignItems: "center" }}>
  <img
    src="/research/images/action_tokenizer.png"
    alt="RVQ."
    style={{ width: "90%", display: "block", margin: "0 auto", marginBottom: "0.1rem" }}
  />
  <figcaption style={{ marginBottom: "1rem", textAlign: "center" }}>
    Learnable Robot Action Tokenizer.
  </figcaption>
</figure>

## Naive Tokenization

Vision-Language-Action (VLA) models rely on naive binning. This involves dividing the continuous action space for each dimension and each timestep into a fixed number of bins. For example, a robot joint angle might be discretized into 256 bins with evenly spaced centers. While simple to implement, this approach has limitations. It is not learnable — the bin boundaries are fixed and do not adapt to the data. Furthermore, it can lead to information loss and may not effectively capture the correlations between consecutive action steps or different action dimensions.

## Learnable Robot Action Tokenization

To overcome the limitations of naive binning, we explored learnable robot action tokenization using vector quantization. The idea is to learn a codebook that optimally represents the continuous action space. By training a vector quantizer, we can compress the continuous action signals into a sequence of discrete tokens that capture the essential information for robot control. Contrary to recently proposed FAST tokenizer, we aim at the learnable approach, following MiniVLA scheme optimizing for efficiency and scalability.
Building on the success of LFQ in other modalities, we applied it to robot actions. Specifically, we extended the concept of action chunks. Instead of tokenizing individual action steps in isolation, we tokenize chunks of future actions — a short horizon of actions. This allows the tokenizer to capture temporal dependencies and compress multiple future steps into a series of discrete tokens. Think of it as summarizing a short planned sequence of movements into a compact, tokenized representation.

By using LFQ for action chunk tokenization, we aim to achieve a learnable, efficient, and scalable method for representing robot actions. This allows us to train powerful auto-regressive Transformer models that can generate high-frequency action sequences through simple next-token prediction, paving the way for more dexterous and responsive robots.

## LIBERO Evaluation

To evaluate the effectiveness of LFQ-based action tokenization, we conducted experiments on the LIBERO benchmark suite. LIBERO focuses on robot manipulation tasks inspired by human activities, using a Franka Emika robot arm. We chose the Object and Spatial subtasks, which represent different challenges in object manipulation and spatial reasoning.
We compared our LFQ action tokenizer to a Residual Vector Quantization (RVQ) tokenizer, a hierarchical VQ approach that has shown promise in action tokenization. We integrated both tokenizers into a Vision-Language-Action model, building upon the EdgeVLA and MiniVLA architectures.

<figure style={{ display: "flex", flexDirection: "column", alignItems: "center" }}>
  <img
    src="/research/images/libero_eval.png"
    alt="RVQ."
    style={{ width: "90%", display: "block", margin: "0 auto", marginBottom: "0.1rem" }}
  />
  <figcaption style={{ marginBottom: "1rem", textAlign: "center" }}>
    Success rate of a VLA model with RVQ or LFQ action chunk tokenization on Libero Object and
    Spatial tasks respectively
  </figcaption>
</figure>

Our results on LIBERO shows that our action tokenizer performed comparably to RVQ on average in the Spatial subtask. While it showed slightly worse performance in the Object domain in our initial experiments, we believe this gap can be closed with further optimization, particularly by exploring larger codebook sizes and potentially simplifying the architecture to a single-layer quantization. Crucially, our approach offers the potential for faster inference compared to RVQ due to the elimination of codebook lookups.

Here are some successful runs:

<div style={{ display: 'flex', gap: '3rem', justifyContent: 'center' }}>
  <figure style={{ textAlign: 'center' }}>
    <img src="/research/images/success_libero_1.gif" alt="GIF 1" style={{ width: '200px', height: 'auto' , marginBottom: "1rem"  }} />
    <figcaption style={{ marginBottom: "1rem", textAlign: "center" }}>Pick up the alphabet soup and place it in the basket.</figcaption>
  </figure>

  <figure style={{ textAlign: 'center' }}>
    <img src="/research/images/success_libero_2.gif" alt="GIF 2" style={{ width: '200px', height: 'auto' , marginBottom: "1rem"  }} />
    <figcaption style={{ marginBottom: "1rem", textAlign: "center" }}>Pick up the black bowl on the wooden cabinet and place it on the plate.</figcaption>
  </figure>
</div>

Take also a look at the less fortunate outcomes:

<div style={{ display: 'flex', gap: '3rem', justifyContent: 'center' }}>
  <figure style={{ textAlign: 'center' }}>
    <img src="/research/images/failure_libero_1.gif" alt="GIF 1" style={{ width: '200px', height: 'auto' , marginBottom: "1rem"  }} />
    <figcaption style={{ marginBottom: "1rem", textAlign: "center" }}>Pick up the black bowl next to the ramekin and place it on the plate.</figcaption>
  </figure>

  <figure style={{ textAlign: 'center' }}>
    <img src="/research/images/failure_libero_2.gif" alt="GIF 2" style={{ width: '200px', height: 'auto' , marginBottom: "1rem"  }} />
    <figcaption style={{ marginBottom: "1rem", textAlign: "center" }}>Pick up the salad dressing and place it in the basket.</figcaption>
  </figure>
</div>

## Conclusions

Efficient and high-quality tokenization is becoming a cornerstone of modern robot learning. A unified tokenizer capable of handling diverse modalities like sound, vision, language, and actions is essential for building truly versatile and scalable robotic systems.
Our continuation of work on effective quantization is encouraging. We’ve demonstrated that it can achieve comparable performance to residual quantization while offering potential advantages in efficiency and scalability. Moving forward, we are excited to explore larger codebook sizes, refine the architecture, and rigorously evaluate our approach on a wider range of robotic platforms and real-world scenarios.
We are also keen to investigate the synergy between our action and speech tokenizers, leveraging multimodal datasets to train robots that can seamlessly integrate auditory and action cues.
